---
layout: default
title: Deep Dive
---

# Deep Dive

In this section, I will post my exploration of various concepts in deep learning.


<div class="link-section">
        <a href="{{ site.baseurl }}/deepdive/ResidualConnections.html">Residual Connections</a>
        <p>Residual links, also called skip connections, allow the input to skip over some layers and be added to the output of those layers. This helps address the degradation problem, where adding more layers to a neural network can actually decrease its performance.</p>
    </div>


<div class="link-section">
        <a href="{{ site.baseurl }}/deepdive/Bert.html">Residual Connections</a>
        <p>BERT (Bidirectional Encoder Representations from Transformers) is the language model based on the encoder part of the transformer architecture. BERT was introduced by Google AI in 2018. It utilizes bidirectional self-attention, allowing it to compute attention from both directions simultaneously. This bidirectional approach enables BERT to capture context from both the left and right sides of a given word, resulting in a more comprehensive understanding of the language. </p>
    </div>

